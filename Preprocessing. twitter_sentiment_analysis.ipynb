{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/aiavorskii/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aiavorskii/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /Users/aiavorskii/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aiavorskii/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "import nltk\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from autocorrect import spell\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "nltk.download('opinion_lexicon')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Loading current instance resource\n",
    "num_partitions = multiprocessing.cpu_count()\n",
    "num_cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', encoding = \"ISO-8859-1\")\n",
    "test = pd.read_csv('test.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbreviation_replacement(text):\n",
    "    text = (text.replace(r\"i'm\", \"i am\")\n",
    "        .replace(r\"'re\", \" are\")\n",
    "        .replace(r\"he's\", \"he is\")\n",
    "        .replace(r\"it's\", \"it is\")\n",
    "        .replace(r\"that's\", \"that is\")\n",
    "        .replace(r\"who's\", \"who is\")\n",
    "        .replace(r\"what's\", \"what is\")\n",
    "        .replace(r\"n't\", \" not\")\n",
    "        .replace(r\"'ve\", \" have\")\n",
    "        .replace(r\"'d\", \" would\")\n",
    "        .replace(r\"'ll\", \" will\")\n",
    "        .replace(r\",\", \" , \")\n",
    "        .replace(r\"!\", \" ! \")\n",
    "        .replace(r\".\", \" . \")\n",
    "        .replace(r\"(\", \" ( \")\n",
    "        .replace(r\")\", \" ) \")\n",
    "        .replace(r\"?\", \" ? \"))\n",
    "    return text\n",
    "\n",
    "def replace_specific_symbols(text):\n",
    "    text = re.sub(r'(.*)&lt;([-])+(.*)', r'\\1 \\3', text).replace('&quot;', '')\n",
    "    return text.lower()\n",
    "\n",
    "def emphasize_pos_and_neg_words(text):\n",
    "    t = []\n",
    "    for w in text.split():\n",
    "        if w in positive_word_library:\n",
    "            t.append('<positive>')\n",
    "        elif w in negative_word_library:\n",
    "            t.append('<negative>')\n",
    "        else:\n",
    "            t.append(w)\n",
    "    newTweet = \" \".join(t)\n",
    "    return newTweet\n",
    "\n",
    "def remove_usernames(text):\n",
    "    text = \" \".join(filter(lambda x:x[0] != '@', text.split()))\n",
    "    return text\n",
    "# TODO: process shortcuts, dunno, 2day, 4, 4ever\n",
    "\n",
    "def get_emoji_options():\n",
    "    loves = [\"<3\", \"â™¥\"]\n",
    "    smilefaces = []\n",
    "    sadfaces = []\n",
    "    neutralfaces = []\n",
    "\n",
    "    eyes = [\"8\",\":\",\"=\",\";\"]\n",
    "    nose = [\"'\",\"`\",\"-\",\"\\\\\"]\n",
    "    for e in eyes:\n",
    "        for n in nose:\n",
    "            for s in [\")\", \"d\", \"]\", \"}\",\"p\"]:\n",
    "                smilefaces.append(e+n+s)\n",
    "                smilefaces.append(e+s)\n",
    "            for s in [\"(\", \"[\", \"{\"]:\n",
    "                sadfaces.append(e+n+s)\n",
    "                sadfaces.append(e+s)\n",
    "            for s in [\"|\", \"/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(e+n+s)\n",
    "                neutralfaces.append(e+s)\n",
    "            #reversed\n",
    "            for s in [\"(\", \"[\", \"{\"]:\n",
    "                smilefaces.append(s+n+e)\n",
    "                smilefaces.append(s+e)\n",
    "            for s in [\")\", \"]\", \"}\", \"/\"]:\n",
    "                sadfaces.append(s+n+e)\n",
    "                sadfaces.append(s+e)\n",
    "            for s in [\"|\", \"/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(s+n+e)\n",
    "                neutralfaces.append(s+e)\n",
    "\n",
    "    smilefaces = list(set(smilefaces))\n",
    "    sadfaces = list(set(sadfaces))\n",
    "    neutralfaces = list(set(neutralfaces))\n",
    "    return loves,smilefaces,sadfaces,neutralfaces\n",
    "\n",
    "loves,smilefaces,sadfaces,neutralfaces = get_emoji_options()\n",
    "\n",
    "def emoji_translation(text):\n",
    "    t = []\n",
    "    for w in text.split():\n",
    "        if w in loves:\n",
    "            t.append(\"<positive>\")\n",
    "#             t.append(\"<love>\")\n",
    "        elif w in smilefaces:\n",
    "            t.append(\"<positive>\")\n",
    "            #t.append(\"<happy>\")\n",
    "        elif w in neutralfaces:\n",
    "            t.append(\"<positive>\")\n",
    "#             t.append(\"<neutral>\")\n",
    "        elif w in sadfaces:\n",
    "            t.append(\"<negative>\")\n",
    "#             t.append(\"<sad>\")\n",
    "        else:\n",
    "            t.append(w)\n",
    "    newText = \" \".join(t)\n",
    "    return newText\n",
    "\n",
    "def replace_multiple_dots(text):\n",
    "    text = '. '.join(re.split(\"\\.+\", text))\n",
    "    return text\n",
    "\n",
    "def extract_hashtag(text):\n",
    "    hash_list = ([re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\")])\n",
    "    return \" \".join(hash_list)\n",
    "\n",
    "def correct_spelling_errors(text):\n",
    "    \"\"\"Delete repeated symbols in every word from text up to 2, apply spelling correction.\"\"\"\n",
    "    return ' '.join(spell(re.sub(r'(.)\\1+', r'\\1\\1', word)) for word in text.split())\n",
    "\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = text.split()\n",
    "    for word in tokens:\n",
    "        if word in stoplist:\n",
    "            tokens.remove(word)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Initialize NLTK function\n",
    "stemmer = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "#  Lemmatization\n",
    "def lemmatize_word(w):\n",
    "    try:\n",
    "        x = lemma.lemmatize(w).lower()\n",
    "        return x\n",
    "    except Exception as e:\n",
    "        return w\n",
    "\n",
    "\n",
    "def lemmatize_sentence(text):\n",
    "    x = [lemmatize_word(t) for t in text.split()]\n",
    "    return \" \".join(x)\n",
    "\n",
    "\n",
    "# Stemming\n",
    "def stemming_word(w):\n",
    "    return stemmer.stem(w)\n",
    "\n",
    "\n",
    "def stemming_sentence(text):\n",
    "    x = [stemming_word(t) for t in text.split()]\n",
    "    return \" \".join(x)\n",
    "\n",
    "def process_row(row):\n",
    "    row = replace_specific_symbols(row)\n",
    "    row = abbreviation_replacement(row)\n",
    "    row = remove_usernames(row)\n",
    "    row = emoji_translation(row)\n",
    "    row = replace_multiple_dots(row)\n",
    "    row = correct_spelling_errors(row)\n",
    "    row = remove_stopwords(row)\n",
    "    row = emphasize_pos_and_neg_words(row)\n",
    "    row = stemming_sentence(row)\n",
    "    row = lemmatize_sentence(row)\n",
    "    return row.strip().lower()\n",
    "\n",
    "def extract_features_subjectivity(text):\n",
    "    sent          = TextBlob(text)\n",
    "    # The polarity score is a float within the range [-1.0, 1.0]\n",
    "    # where negative value indicates negative text and positive\n",
    "    # value indicates that the given text is positive.\n",
    "    #     polarity      = sent.sentiment.polarity\n",
    "    # The subjectivity is a float within the range [0.0, 1.0] where\n",
    "    # 0.0 is very objective and 1.0 is very subjective.\n",
    "    subjectivity  = sent.sentiment.subjectivity\n",
    "#     sent          = TextBlob(text, analyzer = NaiveBayesAnalyzer())\n",
    "#     classification= sent.sentiment.classification\n",
    "#     positive      = sent.sentiment.p_pos\n",
    "#     negative      = sent.sentiment.p_neg\n",
    "    return subjectivity\n",
    "\n",
    "def extract_features_classification(text):\n",
    "#     sent          = TextBlob(text)\n",
    "    # The polarity score is a float within the range [-1.0, 1.0]\n",
    "    # where negative value indicates negative text and positive\n",
    "    # value indicates that the given text is positive.\n",
    "#     polarity      = sent.sentiment.polarity\n",
    "    # The subjectivity is a float within the range [0.0, 1.0] where\n",
    "    # 0.0 is very objective and 1.0 is very subjective.\n",
    "#     subjectivity  = sent.sentiment.subjectivity\n",
    "    sent          = TextBlob(text, analyzer = NaiveBayesAnalyzer())\n",
    "    classification= sent.sentiment.classification\n",
    "#     positive      = sent.sentiment.p_pos\n",
    "#     negative      = sent.sentiment.p_neg\n",
    "    return classification\n",
    "    \n",
    "def apply_processing(data):\n",
    "    data['SentimentText'] = data['SentimentText'].apply(lambda x: process_row(x))\n",
    "#     data['HashTag'] = data['SentimentText'].apply(lambda x: extract_hashtag(x))\n",
    "#     data = data.apply(extract_features, axis=1)\n",
    "    data['subjectivity'] = data['SentimentText'].apply(extract_features_subjectivity)\n",
    "    data['classification'] = data['SentimentText'].apply(extract_features_classification)\n",
    "    return data\n",
    "\n",
    "# processed_train = apply_processing(train)\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = parallelize_dataframe(test, apply_processing)\n",
    "x_test.to_pickle(\"test_clean.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = parallelize_dataframe(train, apply_processing)\n",
    "x_train.to_pickle(\"train_clean.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'just'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_spelling_errors(re.sub(r'(.)\\1+', r'\\1\\1', \"juuuuuuuuuuuuuuuuussssst\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
